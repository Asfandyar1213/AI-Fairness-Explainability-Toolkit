{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFET Visualization Demo\n",
    "This notebook demonstrates the visualization capabilities of the AI Fairness and Explainability Toolkit (AFET)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install plotly pandas scikit-learn numpy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import AFET visualization components\n",
    "from afet.viz import (\n",
    "    FairnessDashboard,\n",
    "    ThresholdAnalysis,\n",
    "    plot_confusion_matrices\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data\n",
    "Create a synthetic dataset with known biases for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 2000\n",
    "\n",
    "# Generate sensitive attribute (e.g., gender)\n",
    "sensitive = np.random.choice(['Male', 'Female'], size=n_samples, p=[0.6, 0.4])\n",
    "\n",
    "# Create synthetic features with some bias\n",
    "n_features = 10\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add bias to the sensitive attribute\n",
    "X[sensitive == 'Male', 0] += 0.5  # Feature 0 is more predictive for males\n",
    "X[sensitive == 'Female', 1] += 0.5  # Feature 1 is more predictive for females\n",
    "\n",
    "# Generate target with some bias\n",
    "coef = np.random.randn(n_features)\n",
    "coef[0] = 1.5  # Make feature 0 more important\n",
    "coef[1] = 1.2  # Make feature 1 more important\n",
    "\n",
    "# Add bias based on sensitive attribute\n",
    "sensitive_effect = np.where(sensitive == 'Male', 0.3, -0.3)\n",
    "y_prob = 1 / (1 + np.exp(-(X @ coef + sensitive_effect + np.random.randn(n_samples) * 0.5)))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "    X, y, sensitive, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Dataset size: {len(X)}\")\n",
    "print(f\"Positive class ratio: {y.mean():.2f}\")\n",
    "print(f\"Sensitive attribute distribution:\\n{pd.Series(sensitive).value_counts(normalize=True).to_string()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fairness Dashboard\n",
    "Create an interactive dashboard to analyze model fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize dashboard\n",
    "dashboard = FairnessDashboard(\n",
    "    model_names=['RandomForest'],\n",
    "    sensitive_attr='Gender'\n",
    ")\n",
    "\n",
    "# Create dashboard\n",
    "fig = dashboard.create_dashboard(\n",
    "    y_true=y_test,\n",
    "    y_pred={'RandomForest': y_pred},\n",
    "    y_prob={'RandomForest': y_prob},\n",
    "    sensitive=sensitive_test,\n",
    "    feature_names=[f'feature_{i}' for i in range(n_features)],\n",
    "    feature_importances={'RandomForest': model.feature_importances_}\n",
    ")\n",
    "\n",
    "# Show dashboard\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threshold Analysis\n",
    "Analyze the trade-off between model performance and fairness across different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create threshold analysis\n",
    "fig = ThresholdAnalysis.plot_threshold_analysis(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    sensitive=sensitive_test,\n",
    "    model_name='RandomForest'\n",
    ")\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrices\n",
    "Compare confusion matrices across different models or sensitive groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a second model for comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model2 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "# Create confusion matrices\n",
    "fig = plot_confusion_matrices(\n",
    "    y_true=y_test,\n",
    "    y_pred_dict={\n",
    "        'RandomForest': y_pred,\n",
    "        'LogisticRegression': y_pred2\n",
    "    },\n",
    "    model_names=['RandomForest', 'LogisticRegression'],\n",
    "    class_names=['Rejected', 'Approved']\n",
    ")\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Model Biases\n",
    "Compare model performance and fairness metrics across sensitive groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from afet.core.fairness_metrics import FairnessMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate metrics for each model\n",
    "metrics = []\n",
    "\n",
    "for name, preds in [('RandomForest', y_pred), ('LogisticRegression', y_pred2)]:\n",
    "    fm = FairnessMetrics(\n",
    "        y_true=y_test,\n",
    "        y_pred=preds,\n",
    "        sensitive_features=sensitive_test\n",
    "    )\n",
    "    \n",
    "    metrics.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': fm.accuracy(),\n",
    "        'Precision': fm.precision(),\n",
    "        'Recall': fm.recall(),\n",
    "        'F1': fm.f1_score(),\n",
    "        'Demographic Parity': fm.demographic_parity_difference(),\n",
    "        'Equal Opportunity': fm.equal_opportunity_difference(),\n",
    "        'Average Odds': fm.average_odds_difference()\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(metrics)\n",
    "results_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Display the results\n",
    "results_df.style\n",
    "    .format('{:.3f}')\n",
    "    .background_gradient(cmap='viridis', axis=0)\n",
    "    .set_caption('Model Performance and Fairness Metrics')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving Visualizations\n",
    "Save the visualizations for reports or presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Save dashboard\n",
    "dashboard_fig = dashboard.create_dashboard(\n",
    "    y_true=y_test,\n",
    "    y_pred={'RandomForest': y_pred},\n",
    "    y_prob={'RandomForest': y_prob},\n",
    "    sensitive=sensitive_test,\n",
    "    feature_names=[f'feature_{i}' for i in range(n_features)],\n",
    "    feature_importances={'RandomForest': model.feature_importances_}\n",
    ")\n",
    "dashboard_fig.write_html('output/fairness_dashboard.html')\n",
    "\n",
    "# Save threshold analysis\n",
    "threshold_fig = ThresholdAnalysis.plot_threshold_analysis(\n",
    "    y_true=y_test,\n",
    "    y_prob=y_prob,\n",
    "    sensitive=sensitive_test,\n",
    "    model_name='RandomForest'\n",
    ")\n",
    "threshold_fig.write_html('output/threshold_analysis.html')\n",
    "\n",
    "# Save confusion matrices\n",
    "confusion_fig = plot_confusion_matrices(\n",
    "    y_true=y_test,\n",
    "    y_pred_dict={\n",
    "        'RandomForest': y_pred,\n",
    "        'LogisticRegression': y_pred2\n",
    "    },\n",
    "    model_names=['RandomForest', 'LogisticRegression'],\n",
    "    class_names=['Rejected', 'Approved']\n",
    ")\n",
    "confusion_fig.write_html('output/confusion_matrices.html')\n",
    "\n",
    "print('Visualizations saved to the output directory.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "This notebook demonstrated the visualization capabilities of the AFET library, including:\n",
    "- Interactive fairness dashboards\n",
    "- Threshold analysis for fairness-accuracy trade-offs\n",
    "- Confusion matrix comparison\n",
    "- Comprehensive model evaluation\n",
    "\n",
    "These tools can help identify and mitigate biases in machine learning models, leading to fairer and more transparent AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
